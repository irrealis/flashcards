defaults:
  deckName: Staging
  modelName: CodingProblems
  extraTags:
  - Code
  - Algorithms
  - Books
  - AFI-AlgorithmsForInterviews
  - AFI-02-Sorting
  - AFI-02a-General
  fields:
    Python: "**To-do: Python.**"
    Java: "**To-do: Java.**"
    C++: "**To-do: C++.**"
  markdownTabLength: 2
notes:
- id: 1536203068476
  modelName: BasicMathJax
  tags:
  - Done-Basic
  - KnowledgeWeakness
  - Marked
  - Mark-NeedsCleanup
  fields:
    Front: |
      ## 02.01 Good sorting algorithms

      What is the most efficient sorting algorithm for each of the following situations:

      - A small array of integers.
      - A large array whose entries are random numbers.
      - A large collection of integers that is already almost sorted.
      - A large collection of integers that are drawn from a very small range.
      - A large collection of numbers most of which are duplicates.
      - Stability is required, i.e., the relative order of two records that have the same sorting key should not be changed.

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 23-4.
    Back: |
      - A small array of integers:
        - Most online sorting algorithms would work.
        - For very small arrays the \\(O(n^2)\\) insertion sort may actually be faster than recursive \\(O(n \log n)\\) sorts, because the former doesn't incur the stack-setup/-teardown overhead of a recursive sort.
        - I personally like merge sort for its simplicity and stability. It isn't in-place, but that shouldn't matter for small-array inputs.

      - A large array whose entries are random numbers.
        - If the size is very large, but in-memory, I'd go with an in-place sort. Since the input is random, a quicksort should perform well. This would be an \\(O(n \log n)\\) sort.
        - If the the array is too large to fit into memory, I'd go with an offline sort. I'd split the array into \\(k\\) roughly equally-sized chunks that each fit into memory, and apply quicksort to each, saving the result to a temporary file; then I'd open \\(k\\) input streams, one for each file, and perform a \\(k\\)-way merge to a single output stream for the result. Technically the sort time would be dominated by \\(O(n \log n)\\), but I think only for extremely large \\(n\\) (and then only if \\(k\\) is so large that \\(k\\) streams cannot be open at once, so would require multiple merge iterations). Practically, the sort time would be likely be dominated by \\(O(n)\\) due to I/O latency, which is several orders of magnitude higher than in-memory operation latency.

      - A large collection of integers that is already almost sorted.
        - An insertion sort or heapsort may work well, if the array is sufficiently "almost sorted". Sort time should approach \\(O(n)\\).
        - More generally, a quicksort with a random pivot should work well, as would a mergesort or heapsort. Run time would be \\(O(n \log n)\\).
        - **Knowledge weakness:** the authors point out that if every element is known to be at most \\(k\\) places from its sorted location, then a min heap can be used for an \\(O(n \log_2 k)\\) sort. This didn't occur to me, revealing my lack of familiarity with heapsort nuances.

      - A large collection of integers that are drawn from a very small range.
        - If the range is bounded by \\(j\\) below and \\(k\\) above, then a bucket sort with \\(k-j+1\\) buckets would work well, and would run in \\(O(n)\\) time.
        - For unbounded input, a bucket sort with a dynamic number of buckets should work well. These buckets could be stored in a BST.

      - A large collection of numbers most of which are duplicates.
        - If the count of distinct possible numbers is small enough, a bucket sort with dynamic number of buckets, stored in a BST, should work well.
        - Otherwise, if the input is in-memory, quicksort (with a good partitioning method that doesn't devolve to \\(O(n^2)\\) on duplicate inputs) or heapsort. Merge sort too, if there's room in memory for an out-of-place sort.

      - Stability is required, i.e., the relative order of two records that have the same sorting key should not be changed.
        - A stable merge sort.
        - Stability can also be achieved by adding to each record a tiebreaker key whose values enumerate the elements in original order, and using a tiebreaking comparator.
- id: 1536203068504
  tags:
  - Done-Python
  fields:
    Front: |
      ## 02.02: TeraSort

      The sorting algorithms alluded to above assume that all the data you need to sort will fit in RAM. What if your data will not fit in memory?

      Sort a file containing \\(10^{12}\\) 100-byte strings.

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 24.
    Python: |
      Rough idea:
      - Split data to chunks each sortable in RAM.
      - Sort each chunk, saving results to disk.
      - Create input streams, one for each saved chunk.
      - Create output stream to save final sorted data.
      - Merge input streams to output stream.

      Issues and assumptions:
      - Distribution of 100-byte strings may not be known.
        - Otherwise, as AfI authors point out, it would be possible to bin each datum to chunks, sort each chunk, then concatenate chunks for result. This is faster but risky unless distribution is guaranteed, rather than approximate.
      - Assume stable sort required.
      - OS/filesystem limits on number of open streams; assume there are "enough".
        - If can't open enough, can be addressed with multiple iterations.

      Actual numbers:
      - Assume 64GB RAM. For safety, will limit memory requirements to approximately 32GB.
      - Assume per-chunk mergesort for stability, so will need room for out-of-place sort; can then mergesort 16GB at a time. Call it 10GB.
      - We have 100TB to sort, so we'll need 10,000 chunks. Well within limits of current Linux kernels for desktops/servers (my computer's upper limit is about three million file descriptors).

      The file i/o will be awful. My system typically maxes out at around 125MB/s for one file. Let's be pessimistic and guess approximately 25MB/s for the merge. So this would take perhaps 50 days to read and sort chunks from the original file, and another 100 to merge the interim files.

      Using a fast SSD NVMe M.2 drive could bring initial chunk sorts down a few days. Using multiple such drives on four different buses could further reduce this to less than a day. I think it might then be feasible to merge in another five to ten days. So perhaps one to two weeks for the entire sort.


      ### Code:

      First I model chunking the data, mergesorting the chunks, and loading input streams:

      ```{python }
      import random

      from irrealis.sorts.mergesort import mergesort

      # https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks
      def make_chunks(l, j):
        for i in range(0, len(l), j):
          yield l[i:i+j]

      def split_to_chunks(l, k):
        j = len(l)//k + 1
        for chunk in make_chunks(l, j):
          yield chunk

      words = open('/usr/share/dict/american-english', 'r').readlines()
      random.shuffle(words)
      chunks = split_to_chunks(words, 10)
      streams = (mergesort(chunk) for chunk in chunks)
      ```

      I'll merge these streams using the following functions:

      ```{python }
      from heapq import heappush, heappop

      def trypush(heap, i, it):
        try: heappush(heap, (next(it), i, it))
        except StopIteration: pass

      def merge(out, *l):
        heap = []
        for i, a in enumerate(l): trypush(heap, i, iter(a))
        while heap:
          x, i, it = heappop(heap)
          out.put(x)
          trypush(heap, i, it)
        return out
      ```

      I model the merge below, starting with a model output stream:

      ```{python }
      import io

      class StringIOStream(io.StringIO):
        def put(self, object):
          return self.write(object)

      outfile = StringIOStream()
      ```

      I merge the input streams to the output stream:

      ```{python }
      merge(outfile, *streams)
      ```

      I check the toy results below:

      ```{python }
      sorted = outfile.getvalue().split()
      print("number of sorted words:", len(sorted))
      print("first twenty sorted words:")
      print(sorted[:20])
      ```

- id: 1536203068551
  fields:
    Front: |
      ## 02.03 Finding the winner and runner-up

      There are 128 players participating in a tennis tournament. Assume that the "\\(x\\) beats \\(y\\)" relationship is transitive, i.e., for all players A, B, and C, if A beats B and B beats C, then A beats C.

      What is the least number of matches we need to organize to find the best player? How many matches do you need to find the best and the second best player?

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 24.
- id: 1536203068587
  fields:
    Front: |
      ## 02.04 Finding the min and max simultaneously

      Given a set of numbers, you can find either the min or max of the set in \\(N-1\\) comparisoms each. whm you need to find both, can you do better than \\(2N-3\\) comparisons?

      Find the min and max elements from a set of \\(N\\) elements using no more than \\(\frac{3N}{2}-1\\) comparisons.

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 24.
- id: 1536203068637
  fields:
    Front: |
      ## 02.05 Efficient trials

      You are the coach of a cycling team with 25 members and need to determine the fastest, second-fastest, and third-fastest cyclists for selection to the Olympic team.

      You will be evaluating the cyclists using a time-trial course on which only 5 cyclists can race at a time. You can use the completion times from a time-trial to rank the 5 cyclists amongst themselves --- no ties are possible. Because conditions can change over time, you cannot compare performances across different time-trials. The relative speeds of cyclists does not change --- if A beats B in one time-trial and B beats C in another time-trial, then A is guaranteed to beat C if they are in the same time-trial.

      What is the minimum number of time-trials needed to determine who to send to the Olympics?

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 24-5.
- id: 1536203068671
  fields:
    Front: |
      ## 02.06 Least-distance sorting

      You come across a collection of 20 stone statues in a line. You want to sort them by height, with the shortest statue on the left. The statues are very heavy and you want to move them the least possible distance.

      Design a sorting algorithm that minimizes the total distance that the statues are moved.

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 25.
- id: 1536203068737
  fields:
    Front: |
      ## 02.07 Privacy and anonymization

      The Massachusetts Group Insurance Commission had a bright idea back in the mid 1990s -- it decided to release "anonymized" data on state employees that showed every single hospital visit they had. The goal was to help the researchers. The state spent time removing identifiers such as name, address, and social security number. The Governor of Massachusetts assured the public that this was sufficient to protect patient privacy. Then a graduate student, Latanya sweeney, saw significant pitfalls in this approach. She requested a copy of the data and by collating the data multiple columns, she was able to identify the health records of the Governor. This demonstrated that extreme care needs to be taken in anonymizing data. One way of ensuring privacy is to aggregate data such that any record can be mapped to at least \\(k\\) individuals, for some large value of \\(k\\).

      Suppose you are given a matrix \\(M\\), where each row represents an individual and each column represents m attribute about the individual such as age or gender. Given a set of columns to be deleted, you want to determine if each row has at least \\(k\\) duplicate rows with exactly the same contents in the remaining columns. How would you verify this efficiently?

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 25-6.
- id: 1536203068791
  fields:
    Front: |
      ## 02.08 Variable length sort

      Most sorting algorithms rely on a basic swap step. When records are of different lengths, the swap step becomes nontrivial.

      Sort lines of a text file that has a million lines such that the average length of a line is 100 characters but the longest line is one million characters long.

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 26.
- id: 1536203068822
  fields:
    Front: |
      ## 02.09 Unique elements

      Suppose you are given a set of names and your job is to produce a set of unique first names. If you just remove the last name from all the names you may have some duplicate first names.

      How would you create a set of first names that has each name occurring only once? Specifically, design an efficient algorithm for removing all the duplicates from an array.

      --- Aziz and Prakash; _Algorithms for Interviews_; version 1.0.0 (September 1, 2010); p 26.
